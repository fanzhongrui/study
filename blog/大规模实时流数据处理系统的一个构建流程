之前在《程序员2013精华本》上看到一篇雅虎精准广告投放系统的文章，回忆了一下，总结一篇大规模实时流数据处理系统的一个构建流程。
应用场景

    该系统处理用户在雅虎网站上的浏览、点击、搜索等事件，实时获取为该用户提供服务的雅虎服务器所在的数据中心，并将这些信息转换成内部编码发送给广告服务系统数据库；同时，为了将这些信息分享给其他系统，以对它们进行持久化存储。
该系统流水线根据用户的历史行为计算出用户的属性，然后将用户属性发送给广告服务器数据库。广告服务系统根据数据库中的用户分类属性精准地投放广告。
    理想情况下，为了保证广告服务系统能以最快的速度获得用户属性从而更快更精确的给予用户响应，广告服务系统会将用户属性复制到全球的数据中心中存储。但事实上，为了节省存储和跨数据中心的数据交互，只需要将用户属性复制到离用户最近的数据中心，并且当用户所在区域改变时最近的数据中心也会变化，这就要根据用户属性实时选择。
系统架构

    实时流数据处理系统，包括一个分布式消息队列，用来接收数据采集服务器发送过来的用户事件；一个storm topology，用来分布式处理用户事件，抽取特征（数据中心的名称），计算用户属性（数据中心对应的内部编码），并将最终结果写入持久层和广告服务系统；一个数据持久层服务，用于持久化存储用户属性。

数据源和消息队列

    Storm允许用户定制Spout从外部数据源读取数据。同时storm提供了从一些分布式式消息队列（如Kestrel）Spout。该系统将用户在web服务器上的活动日志以事件的方式传递给后台。后台的数据中心接收到事件后，将事件写入Kestrel队列，然后storm从Kestrel队列中读取事件并完成数据处理。
Trident Topology
    Topology设计如下：
    一个Spout：从对应的Kestrel队列读取事件数据，并以shuffle grouping的方式发送给特征抽取bolt；
    一个特征抽取bolt：以无状态方式抽取用户特征；
    一个用于计算用户属性的bolt：根据用户特征计算用户属性，将用户属性持久化存储到数据库，并将用户属性发送给下游系统（广告服务系统）。
若不使用Trident，需要显式地构建特征抽取bolt和分类bolt。通过Trident，用下图中的伪代码创建Trident Topology，然后Trident计算出最优的Topology（类似Pig的查询优化功能）

 

    该系统采用HBase作为数据持久层。为了保证读的性能，为了分类器bolt设计了进程内LRU缓存。由于用户特征在从特征抽取bolt发送到分类器bolt采用的 是field grouping的方式，因此同样的用户id总会被发送到同样的进程，除非Storm Topology被重新平衡，不然cache不会失效。
数据输出
    数据还会被输出到广告服务系统的数据库。为了避免并发写数据库，将用户属性写入到一个Kestrel队列，由广告服务系统从该队列读取用户属性并写入数据库。
 
    实时处理系统（类似S4，STORM）对比直接用MQ来做好处在哪里？

    好处是它帮你做了：1）集群控制。2）任务分配。3）任务分发。4）监控等
    storm并不是一个完整的解决方案。使用storm需要加入消息队列做数据入口，考虑如何在流中保存状态，考虑怎样将大问题用分布式去解决。解决这些问题的成本可能比增加一个服务器的成本还高。但是，一旦下定决定使用了storm并解决了那些恼人的细节，就能享受storm带来的简单，可拓展等优势了。
